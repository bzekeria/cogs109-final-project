{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "694a4a58",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "**revise and add to paper**\n",
    "\n",
    "> We initially considered the \\textbf{Bag-of-Words (BoW) model} as a potential feature selection method for our task. The BoW model represents text data by creating a set of unique words, disregarding their order, and counting their occurrences in each document. While the BoW model is a simple and widely used approach, it has certain limitations that led us to explore alternative methods.\n",
    "\n",
    "The BoW model formula calculates the term frequency (TF) for each word in a document. It can be expressed as follows:\n",
    "\n",
    "$$\\text{BOW}(d) = (t_1, \\text{Count}(t_1, d)), (t_2, \\text{Count}(t_2, d)), \\ldots, (t_n, \\text{Count}(t_n, d))$$\n",
    "\n",
    "One limitation of the BoW model is that it does not consider the importance of words within a document or in the entire corpus. It treats all words equally, which may not be ideal for capturing the semantic meaning of the products. Additionally, the BoW model suffers from the \"curse of dimensionality\" since the resulting feature space can be very large, making it computationally expensive and potentially leading to overfitting.\n",
    "\n",
    "To address these limitations, we decided to use the \\textbf{Term Frequency-Inverse Document Frequency (TF-IDF)} as our preferred feature extraction method to numerically represent and analyze the product names in order to classify them into their respective categories. \n",
    "\n",
    "First, the product names are preprocessed by \\emph{tokenizing} them into individual words using \\texttt{nltk.word\\_tokenize}. Then, the words are lemmatized using \\texttt{nltk.WordNetLemmatizer} to obtain their root forms. This preprocessing \n",
    "step ensures consistency and reduces the complexity of the data.\n",
    "\n",
    "TF-IDF is calculated based on two main components: Term Frequency ($TF$) and Inverse Document Frequency ($IDF$).\n",
    "\n",
    "The TF component measures the frequency of a term (word) within a specific product name. It is computed as the number of occurrences of the term in the product name divided by the total number of terms in the product name. Mathematically, the TF formula can be represented as:\n",
    "\n",
    "$$\\text{TF}(\\text{t},~\\text{d}) = \\frac{{\\text{Number~of~occurrences~of~term~}\\text{t}~\\text{in~document~}\\text{d}}}{{\\text{Total~number~of~terms~in~document~}\\text{d}}}$$\n",
    "\n",
    "The IDF component measures the importance of a term across the entire dataset of product names. It considers how many documents (product names) contain the term. IDF is calculated as the logarithm of the total number of documents divided by the number of documents containing the term. The IDF formula can be expressed as:\n",
    "\n",
    "$$\\text{IDF}(\\text{t},~\\text{D}) = \\log \\left( \\frac{{\\text{Total~number~of~documents~in~the~corpus~D}}}{{\\text{Number~of~documents~containing~term~t}}} \\right)$$\n",
    "\n",
    "The $TF-IDF$ value is obtained by multiplying the $TF$ and $IDF$ values for each term in a product name: $\\text{TF-IDF}(\\text{t}, ~\\text{d}, ~\\text{D}) = \\text{TF}(\\text{t}, ~\\text{d}) \\times \\text{IDF}(\\text{t}, ~\\text{D})$. It assigns higher weights to terms that are frequent within the product name but rare across the entire dataset. This captures the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e627b063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nsharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nsharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nsharma/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/nsharma/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# notebook configurations\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94e0e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>main_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47875</th>\n",
       "      <td>ahuja portable pa system bluetooth usb echo effect rechargeable battery black</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                name  \\\n",
       "47875  ahuja portable pa system bluetooth usb echo effect rechargeable battery black   \n",
       "\n",
       "      main_category  \n",
       "47875         music  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/amazon_products_sampled_cleaned.csv\")\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaaebb6",
   "metadata": {},
   "source": [
    "## Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0978ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"word_count\"] = df[\"name\"].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b11acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"char_count\"] = df[\"name\"].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc1acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# think of other stuff?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8592823d",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce5945",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13f742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"name\"], df[\"main_category\"], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de00801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5eaf36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_null_counts = X_train.isnull().sum()\n",
    "X_train_null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "037ce73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72857    NaN\n",
       "75908    NaN\n",
       "24632    NaN\n",
       "59339    NaN\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_null_rows = X_train[X_train.isnull()]\n",
    "X_train_null_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2f7257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dad00ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac1fe4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer on the training data\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "train_words_numerical = pd.DataFrame(train_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "train_words_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e16529c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_null_counts = X_test.isnull().sum()\n",
    "X_test_null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84da2417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: name, dtype: object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_null_rows = X_test[X_test.isnull()]\n",
    "X_test_null_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02de1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34036e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1195256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71603500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the testing data using the fitted vectorizer\n",
    "test_features = vectorizer.transform(X_test)\n",
    "test_words_numerical = pd.DataFrame(test_features.toarray(), columns = vectorizer.get_feature_names())\n",
    "test_words_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ebcb0f",
   "metadata": {},
   "source": [
    "## Discuss\n",
    "\n",
    "- Both the ```train_features``` and ```test_features``` have a **high-dimensional data**  which can be **computationally expensive** for the ML algorithm (see KNN below)\n",
    "- Professor discussed with us some options\n",
    "    - Feature selection?\n",
    "        - Why?\n",
    "            - Our TF-IDF bectorizer above is wayyyy too large for our model to handle\n",
    "            - Allows us to identify the most relevant features (words) and eliminate noise -> reduce overfitting\n",
    "                - Also eliminates irrelevent features ```aaaaacd``` is garbage lol\n",
    "    - Mean TF-IDF by category?\n",
    "        - confused on how to implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 100  # Adjust the number of components as per your needs\n",
    "pca = PCA(n_components=n_components)\n",
    "train_features_reduced = pca.fit_transform(train_words_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_rescaled = scaler.fit_transform(train_features_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f36b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Create a SelectKBest object with the chi2 scoring function\n",
    "k = 600  # Number of top features to select\n",
    "selector = SelectKBest(score_func=chi2, k=k)\n",
    "\n",
    "# Fit the selector to the TF-IDF training data\n",
    "X_train_selected = selector.fit_transform(train_features, y_train)\n",
    "\n",
    "# Feature names\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "selected_features = [features[i] for i in selected_feature_indices]\n",
    "\n",
    "# Transform the TF-IDF test data using the selected features\n",
    "X_test_selected = test_features[:, selected_feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train_selected to a DataFrame\n",
    "df_train_selected = pd.DataFrame(X_train_selected.toarray(), columns=selected_features)\n",
    "df_train_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138907f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_test_selected to a DataFrame\n",
    "df_test_selected = pd.DataFrame(X_test_selected.toarray(), columns=selected_features)\n",
    "df_test_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64887568",
   "metadata": {},
   "source": [
    "## Models\n",
    "- originally we had naive bayes (intuitive)\n",
    "- professor suggested KNN (K-Nearest Neighbors)\n",
    "- we can compare the two and stick with 1 model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84cabf6",
   "metadata": {},
   "source": [
    "**revise and add to paper**\n",
    "\n",
    "### Multinomial Naive Bayes Classifier\n",
    "\n",
    "To find the most probable category $c$ given a document $d$ (product name), we need to calculate $P(c|d)$, the probability of category $c$ given the document $d$. In the **Multinomial Naive Bayes Classifier (MNB)**, this can be done by estimating the conditional probability $P(c|d)$ using **Bayes' theorem**:\n",
    "\n",
    "$$P(c|d) = \\frac{{P(d|c) \\cdot P(c)}}{{P(d)}}$$\n",
    "\n",
    "where $P(d|c)$ is the probability of product $d$ given category $c$, $P(c)$ is the prior probability of category $c$, and $P(d)$ is the probability of product $d$.\n",
    "\n",
    "To estimate $P(d|c)$, we consider the features present in product $d$ and calculate their probabilities given category $c$:\n",
    "\n",
    "$$P(d|c) = P(f_1|c) \\cdot P(f_2|c) \\cdot \\ldots \\cdot P(f_n|c)$$\n",
    "\n",
    "where $f_1, f_2, \\ldots, f_n$ represent the features in product $d$. In the Multinomial Naive Bayes Classifier, we assume that the features follow a multinomial distribution.\n",
    "\n",
    "To calculate $P(f_i|c)$, the probability of feature $f_i$ given category $c$, we can use the following formula:\n",
    "\n",
    "$$P(f_i|c) = \\frac{{\\text{tf-idf}(f_i, d) + \\alpha}}{{\\sum_{f \\in F}(\\text{tf-idf}(f, d) + \\alpha \\cdot |F|)}}$$\n",
    "\n",
    "where $\\text{tf-idf}(f_i, d)$ represents the TF-IDF value of feature $f_i$ in product $d$. The smoothing parameter $\\alpha$ is added to the numerator, ensuring non-zero probabilities for features with zero count. The denominator includes the term $\\alpha \\cdot |F|$ to normalize the probabilities, where $|F|$ is the cardinality of the set of all features. This normalization guarantees that the probabilities sum up to 1 for each category $c$ and balances the probabilities across all features. By incorporating $\\alpha$ and $|F|$, we handle unseen features and achieve well-distributed and normalized probabilities, enhancing the robustness and reliability of the classification results.\n",
    "\n",
    "By employing the TF-IDF methodology and a probability-based classification approach, the project aims to accurately categorize Amazon product sales based on their names. Through calculating probabilities for each category and comparing them, the most likely category for a given product can be determined. This categorization process will enhance organization and searchability on the platform, facilitating improved browsing and product discovery experiences for users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0aa7a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c55b3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "062d0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            accessories       0.78      0.79      0.78       894\n",
      "             appliances       0.85      0.93      0.89       913\n",
      "         bags & luggage       0.69      0.82      0.75       902\n",
      "        beauty & health       0.81      0.82      0.82       852\n",
      "        car & motorbike       0.86      0.87      0.87       869\n",
      "grocery & gourmet foods       0.90      0.91      0.90       610\n",
      "         home & kitchen       0.75      0.78      0.76       910\n",
      "    home, kitchen, pets       0.00      0.00      0.00         6\n",
      "    industrial supplies       0.87      0.75      0.81       807\n",
      "          kids' fashion       0.78      0.72      0.75       897\n",
      "         men's clothing       0.78      0.93      0.85       910\n",
      "            men's shoes       0.79      0.91      0.84       911\n",
      "                  music       0.97      0.31      0.47       232\n",
      "           pet supplies       0.98      0.66      0.79       271\n",
      "       sports & fitness       0.77      0.71      0.74       939\n",
      "                 stores       0.70      0.46      0.55       933\n",
      "   toys & baby products       0.83      0.88      0.85       887\n",
      "    tv, audio & cameras       0.83      0.93      0.88       913\n",
      "       women's clothing       0.87      0.89      0.88       903\n",
      "          women's shoes       0.91      0.92      0.92       878\n",
      "\n",
      "               accuracy                           0.81     15437\n",
      "              macro avg       0.79      0.75      0.75     15437\n",
      "           weighted avg       0.81      0.81      0.80     15437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "report = classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "872f93a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            accessories       0.73      0.73      0.73       894\n",
      "             appliances       0.77      0.82      0.80       913\n",
      "         bags & luggage       0.65      0.75      0.70       902\n",
      "        beauty & health       0.42      0.75      0.53       852\n",
      "        car & motorbike       0.78      0.76      0.77       869\n",
      "grocery & gourmet foods       0.83      0.71      0.77       610\n",
      "         home & kitchen       0.64      0.53      0.58       910\n",
      "    home, kitchen, pets       0.00      0.00      0.00         6\n",
      "    industrial supplies       0.80      0.52      0.63       807\n",
      "          kids' fashion       0.70      0.61      0.65       897\n",
      "         men's clothing       0.71      0.93      0.81       910\n",
      "            men's shoes       0.73      0.90      0.80       911\n",
      "                  music       0.98      0.44      0.61       232\n",
      "           pet supplies       0.97      0.76      0.85       271\n",
      "       sports & fitness       0.73      0.60      0.66       939\n",
      "                 stores       0.61      0.34      0.44       933\n",
      "   toys & baby products       0.75      0.73      0.74       887\n",
      "    tv, audio & cameras       0.83      0.83      0.83       913\n",
      "       women's clothing       0.79      0.84      0.81       903\n",
      "          women's shoes       0.86      0.88      0.87       878\n",
      "\n",
      "               accuracy                           0.72     15437\n",
      "              macro avg       0.71      0.67      0.68     15437\n",
      "           weighted avg       0.73      0.72      0.71     15437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### feature selection implementation\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_selected, y_train)\n",
    "predictions = classifier.predict(X_test_selected)\n",
    "report = classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cab89",
   "metadata": {},
   "source": [
    "**revise and add to paper**\n",
    "\n",
    "### K-Nearest Neighbors (KNN)\n",
    "\n",
    "> The probabilities of each category can be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39315f56",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'nyx professional makeup jumbo eye pencil black bean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2923/2619082546.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 X, y = self._validate_data(X, y, accept_sparse=\"csr\",\n\u001b[0m\u001b[1;32m    364\u001b[0m                                            multi_output=True)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    891\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    892\u001b[0m         \"\"\"\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'nyx professional makeup jumbo eye pencil black bean'"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_features_numerical, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24d39dee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2923/3229112478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Make predictions on the testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#predictions = knn.predict(X_test_selected)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    647\u001b[0m                [2]]...)\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_neighbors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This KNeighborsClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "#Make predictions on the testing data\n",
    "predictions = knn.predict(test_features)\n",
    "#predictions = knn.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02401f",
   "metadata": {},
   "source": [
    "#Evaluate the model\n",
    "report = classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc80c383",
   "metadata": {},
   "source": [
    "#### Visualizations\n",
    "???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
