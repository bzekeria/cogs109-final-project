{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2790c9c",
   "metadata": {},
   "source": [
    "Below is a mock example of our project starting from data cleanig till model evluation. To avoid confusion, mainly look into **feature engineering** which shows how the NLP method we're using, [TF-IDF](https://medium.com/analytics-vidhya/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3#:~:text=TF%2DIDF%20or%20(%20Term%20Frequency,machine%20read%20words%20in%20numbers.), will be implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fee014",
   "metadata": {},
   "source": [
    "pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9f1a6",
   "metadata": {},
   "source": [
    "**revise and add to paper**\n",
    "\n",
    "In this project, the **TF-IDF (Term Frequency-Inverse Document Frequency)** method is utilized to represent and analyze the product names in order to classify them into their respective categories. The TF-IDF approach provides a numerical representation of the text data, allowing for effective analysis and prediction.\n",
    "\n",
    "First, the product names are preprocessed by tokenizing them into individual words using ```nltk.word_tokenize```. Then, the words are lemmatized using ```nltk.WordNetLemmatizer``` to obtain their root forms. This preprocessing step ensures consistency and reduces the complexity of the data.\n",
    "\n",
    "TF-IDF is calculated based on two main components: Term Frequency (TF) and Inverse Document Frequency (IDF).\n",
    "\n",
    "The TF component measures the frequency of a term (word) within a specific product name. It is computed as the number of occurrences of the term in the product name divided by the total number of terms in the product name. Mathematically, the TF formula can be represented as:\n",
    "\n",
    "$$TF(t, d) = \\frac{{\\text{Number of occurrences of term t} \\text{ in document d}}}{{\\text{Total number of terms in document d}}}$$\n",
    "\n",
    "The IDF component measures the importance of a term across the entire dataset of product names. It considers how many documents (product names) contain the term. IDF is calculated as the logarithm of the total number of documents divided by the number of documents containing the term. The IDF formula can be expressed as:\n",
    "\n",
    "$$IDF(t, D) = \\log \\left( \\frac{{\\text{Total number of documents in the corpus D}}}{{\\text{Number of documents containing term t}}} \\right)$$\n",
    "\n",
    "The TF-IDF value is obtained by multiplying the TF and IDF values for each term in a product name: $\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$. It assigns higher weights to terms that are frequent within the product name but rare across the entire dataset. This captures the relative importance of words in distinguishing between different product categories.\n",
    "\n",
    "The probabilities of each category can be computed using a Multinomial Naive Bayes classifier, where the likelihood of a product belonging to a specific category is estimated based on the TF-IDF values. The probability of a product belonging to a category can be calculated using **Bayes' theorem**:\n",
    "\n",
    "$$P(Category | Product) = \\frac{{P(Product | Category) \\cdot P(Category)}}{{P(Product)}}$$\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $P(Category | Product)$: represents the probability of a product belonging to a specific category given its TF-IDF representation.\n",
    "- $P(Product | Category)$: represents the likelihood of a product's TF-IDF values given a specific category. This is estimated using the Multinomial Naive Bayes classifier.\n",
    "- $P(Category)$: represents the prior probability of a category, which can be calculated based on the distribution of categories in the training data.\n",
    "- $P(Product)$: is the evidence probability, which serves as a normalization factor and can be calculated by summing the probabilities over all possible categories.\n",
    "\n",
    "By leveraging the TF-IDF methodology and probability-based classification, the project aims to accurately categorize Amazon product sales based on their names, facilitating improved organization and searchability on the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f471c498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bzekeria/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bzekeria/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/bzekeria/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/bzekeria/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# notebook configurations\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3723c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/amazon_products_sampled_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb711b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df[\"cleaned_name\"] = df[\"cleaned_name\"].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487f898",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b455c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "df[\"cleaned_name\"] = df[\"name\"].apply(lambda x: re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(x)))\n",
    "# convert to lowercase\n",
    "df[\"cleaned_name\"] = df[\"cleaned_name\"].apply(lambda x: x.lower())\n",
    "# Remove words containing a number (brand numbers, product id, etc.)\n",
    "df[\"cleaned_name\"] = df[\"cleaned_name\"].apply(lambda x: ' '.join([word for word in x.split() if not re.search(r'\\d', word)]))\n",
    "# Filter out non-English words\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "df[\"cleaned_name\"] = df[\"cleaned_name\"].apply(lambda x: ' '.join([word for word in x.split() if word in english_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ca8bc",
   "metadata": {},
   "source": [
    "**It may not be a good thing to remove non-English words as the ```NLTK corpus``` doesn't contain all english words. There's other problems as well which can be explored later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401a3539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Leonardi Tie Pin for Men', 'tie pin for men')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"name\"][0], df[\"cleaned_name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8d960c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"cleaned_name\"], df[\"main_category\"], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05297f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edcc8c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aam</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abb</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>aberration</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zircon</th>\n",
       "      <th>zirconia</th>\n",
       "      <th>zirconium</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooter</th>\n",
       "      <th>zyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73286</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73288</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73290</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73291 rows × 11553 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aam  abacus  abb  abbey  abdomen  abdominal  aberration  ability  \\\n",
       "0      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "1      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "2      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "3      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "4      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "...    ...  ...     ...  ...    ...      ...        ...         ...      ...   \n",
       "73286  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "73287  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "73288  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "73289  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "73290  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "\n",
       "       able  ...  zircon  zirconia  zirconium  zodiac  zombie  zone  zoo  \\\n",
       "0       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "1       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "2       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "3       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "4       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "...     ...  ...     ...       ...        ...     ...     ...   ...  ...   \n",
       "73286   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "73287   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "73288   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "73289   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "73290   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "\n",
       "       zoom  zooter  zyme  \n",
       "0       0.0     0.0   0.0  \n",
       "1       0.0     0.0   0.0  \n",
       "2       0.0     0.0   0.0  \n",
       "3       0.0     0.0   0.0  \n",
       "4       0.0     0.0   0.0  \n",
       "...     ...     ...   ...  \n",
       "73286   0.0     0.0   0.0  \n",
       "73287   0.0     0.0   0.0  \n",
       "73288   0.0     0.0   0.0  \n",
       "73289   0.0     0.0   0.0  \n",
       "73290   0.0     0.0   0.0  \n",
       "\n",
       "[73291 rows x 11553 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the vectorizer on the training data\n",
    "train_features = vectorizer.fit_transform(X_train)\n",
    "train_words_numerical = pd.DataFrame(train_features.toarray(), columns=vectorizer.get_feature_names())\n",
    "train_words_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec9ab2",
   "metadata": {},
   "source": [
    "# Get the list of words (vocabulary)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2712c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aam</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abb</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>aberration</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>zircon</th>\n",
       "      <th>zirconia</th>\n",
       "      <th>zirconium</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooter</th>\n",
       "      <th>zyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18318</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18319</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18321</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18323 rows × 11553 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aam  abacus  abb  abbey  abdomen  abdominal  aberration  ability  \\\n",
       "0      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "1      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "2      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "3      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "4      0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "...    ...  ...     ...  ...    ...      ...        ...         ...      ...   \n",
       "18318  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "18319  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "18320  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "18321  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "18322  0.0  0.0     0.0  0.0    0.0      0.0        0.0         0.0      0.0   \n",
       "\n",
       "       able  ...  zircon  zirconia  zirconium  zodiac  zombie  zone  zoo  \\\n",
       "0       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "1       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "2       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "3       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "4       0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "...     ...  ...     ...       ...        ...     ...     ...   ...  ...   \n",
       "18318   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "18319   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "18320   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "18321   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "18322   0.0  ...     0.0       0.0        0.0     0.0     0.0   0.0  0.0   \n",
       "\n",
       "       zoom  zooter  zyme  \n",
       "0       0.0     0.0   0.0  \n",
       "1       0.0     0.0   0.0  \n",
       "2       0.0     0.0   0.0  \n",
       "3       0.0     0.0   0.0  \n",
       "4       0.0     0.0   0.0  \n",
       "...     ...     ...   ...  \n",
       "18318   0.0     0.0   0.0  \n",
       "18319   0.0     0.0   0.0  \n",
       "18320   0.0     0.0   0.0  \n",
       "18321   0.0     0.0   0.0  \n",
       "18322   0.0     0.0   0.0  \n",
       "\n",
       "[18323 rows x 11553 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the testing data using the fitted vectorizer\n",
    "test_features = vectorizer.transform(X_test)\n",
    "test_words_numerical = pd.DataFrame(test_features.toarray(), columns = vectorizer.get_feature_names())\n",
    "test_words_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf355fd",
   "metadata": {},
   "source": [
    "# Create a dictionary to store the vocabulary for each category\n",
    "category_vocabulary = {}\n",
    "\n",
    "categories = train_labels.unique()\n",
    "for category in categories:\n",
    "    \n",
    "    category_data = train_data[train_labels == category]\n",
    "    \n",
    "    category_features = vectorizer.fit_transform(category_data)\n",
    "\n",
    "    words = vectorizer.get_feature_names()\n",
    "    words = [word for word in words if not re.search(r'\\d', word)]\n",
    "    english_words = set(nltk.corpus.words.words())\n",
    "    words = [word for word in words if word in english_words]\n",
    "\n",
    "    category_vocabulary[category] = words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98247bb",
   "metadata": {},
   "source": [
    "category_vocabulary[\"sports & fitness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a4ac6",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d7f2b",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3de4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ee659b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb67074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "            accessories       0.62      0.77      0.69      1062\n",
      "             appliances       0.84      0.91      0.88      1124\n",
      "         bags & luggage       0.68      0.77      0.73      1096\n",
      "        beauty & health       0.79      0.78      0.78      1088\n",
      "        car & motorbike       0.79      0.85      0.82      1096\n",
      "grocery & gourmet foods       0.88      0.86      0.87       653\n",
      "         home & kitchen       0.69      0.75      0.71      1103\n",
      "    home, kitchen, pets       0.00      0.00      0.00         3\n",
      "    industrial supplies       0.85      0.66      0.74       815\n",
      "          kids' fashion       0.71      0.58      0.64      1084\n",
      "         men's clothing       0.71      0.91      0.80      1092\n",
      "            men's shoes       0.70      0.80      0.75      1132\n",
      "                  music       0.96      0.30      0.45       235\n",
      "           pet supplies       0.99      0.65      0.78       310\n",
      "       sports & fitness       0.69      0.65      0.67      1071\n",
      "                 stores       0.60      0.33      0.42      1110\n",
      "   toys & baby products       0.80      0.79      0.80      1095\n",
      "    tv, audio & cameras       0.81      0.90      0.85      1105\n",
      "       women's clothing       0.76      0.82      0.79      1083\n",
      "          women's shoes       0.84      0.76      0.80       966\n",
      "\n",
      "               accuracy                           0.75     18323\n",
      "              macro avg       0.74      0.69      0.70     18323\n",
      "           weighted avg       0.75      0.75      0.74     18323\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "report = classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe96012",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifier (LinearSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043654b",
   "metadata": {},
   "source": [
    "# Initialize and train the classifier\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9276fd2",
   "metadata": {},
   "source": [
    "# Predict labels for the test set\n",
    "predictions = classifier.predict(test_features)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "classification_report(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b336f4",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a10011",
   "metadata": {},
   "source": [
    "# Create a KNN classifier\n",
    "k = 5  # Number of neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the KNN classifier\n",
    "knn.fit(train_features.toarray(), train_labels)\n",
    "\n",
    "# Predict the categories for test data\n",
    "y_pred = knn.predict(test_features)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7f558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
